# Observability


## ğŸ§  1. **Comment choisir les mÃ©triques importantes ?**

### ğŸ¯ Objectif : Identifier les indicateurs de santÃ©, performance, et erreurs de chaque composant.

### ğŸ“Œ **3 types de mÃ©triques clÃ©s :**

| Type              | But                                             | Exemples                          |
|-------------------|--------------------------------------------------|-----------------------------------|
| ğŸ”§ **Health**     | Est-ce que le service est vivant ?              | `up`, `process_cpu_usage`, `pg_up` |
| ğŸƒâ€â™‚ï¸ **Performance** | Est-ce que Ã§a rÃ©pond vite et bien ?             | `http_request_duration_seconds`, `pg_stat_activity` |
| ğŸ”¥ **Errors**      | Est-ce quâ€™il y a des problÃ¨mes ?                | `5xx`, `logback_events_total{level="error"}` |

### âœ… **CritÃ¨res pour sÃ©lectionner une mÃ©trique importante :**

1. **Est-ce quâ€™elle tâ€™aiderait Ã  dÃ©tecter un incident avant lâ€™utilisateur ?**
2. **Est-ce quâ€™elle te permettrait de comprendre un incident plus rapidement ?**
3. **Est-ce quâ€™elle reflÃ¨te un SLA (latence, uptime) ?**
4. **Est-ce quâ€™elle suit une ressource limitÃ©e (CPU, RAM, connexions) ?**
5. **Est-elle corrÃ©lÃ©e Ã  un Ã©vÃ©nement business important ?**

### ğŸ§ª Exemples par service :

- **Java App** :
  - Temps de rÃ©ponse (latency P95)
  - Erreurs HTTP
  - RAM JVM
  - CPU

- **PostgreSQL** :
  - Connexions ouvertes
  - Transactions/s
  - Locks / deadlocks

- **Redis** :
  - MÃ©moire utilisÃ©e
  - ClÃ©s Ã©jectÃ©es
  - Clients connectÃ©s

- **RabbitMQ** :
  - Messages non consommÃ©s
  - Nombre de consommateurs

---

## â± 2. **Comment choisir la pÃ©riode de scrape ?**

### ğŸ¯ But : Ã‰quilibrer fraÃ®cheur des donnÃ©es vs charge rÃ©seau et stockage


| Type de service   | FrÃ©quence recommandÃ©e   |
|-------------------|--------------------------|
| App critique (Java) | `15s` Ã  `30s`            |
| DB / Cache / Broker | `30s` Ã  `60s`            |
| Services peu dynamiques | `60s` ou +             |

### âœ… Comment dÃ©cider ?

- Si une alerte doit dÃ©clencher en **moins dâ€™une minute**, scrape Ã  15â€“30s
- Pour logs ou tendances Ã  long terme : 60s suffit
- Scrape frÃ©quent = + stockage = Ã  compresser avec retention + compaction

**âš ï¸ Conseil** : Mets `scrape_interval` par job dans Prometheus, pas globalement

---

## ğŸ“„ 3. **Comment choisir les logs importants ?**

### ğŸ¯ Objectif : Identifier les Ã©vÃ©nements significatifs Ã  suivre / alerter

1. **Indiquent une erreur** : `level=error`, `exception`, `panic`, etc.
2. **Montrent une dÃ©gradation** : `warn`, timeouts, retries
3. **RÃ©vÃ¨lent une action critique** : `user_deleted`, `payment_failed`
4. **Sont frÃ©quents** : +500 fois dans 1 min = problÃ¨me silencieux
5. **Contiennent des mots-clÃ©s clÃ©s** : "connection refused", "OOM", "deadlock", etc.

### âœ… Filtrer avec **LogQL (Loki)** :

```logql
{job="spring-app", level="error"} |= "Exception"
{job="postgres", level="warn"} |= "deadlock"
```

---

## ğŸ”­ 4. **Concepts clÃ©s pour une observabilitÃ© "production-ready"**

### ğŸŒ 1. **Les 3 piliers de l'observabilitÃ©**

| Pilier      | Objectif                           | Exemple outil |
|-------------|------------------------------------|---------------|
| Metrics     | Comprendre **quoi** ne va pas      | Prometheus    |
| Logs        | Comprendre **pourquoi** Ã§a ne va pas| Loki          |
| Traces (bonus) | Suivre le parcours dâ€™une requÃªte  | Jaeger, OpenTelemetry |

---

### ğŸ”„ 2. **CorrÃ©lation entre logs et mÃ©triques**

> Exemple : Un pic de `5xx` dans les mÃ©triques â†’ Aller voir les logs dâ€™erreurs de la mÃªme minute dans Loki.

Grafana permet Ã§a avec les **"Explore correlations"** sur les dashboards : clique sur une courbe, puis "View logs for this time".

---

### ğŸ“Š 4. **Dashboards pertinents**
- Un par composant/service
- Un global "SRE view" ou "health overview"
- Regroupe :
  - Latence
  - Erreurs
  - Ressources systÃ¨me
  - Logs critiques

---

## âœ… RÃ©capitulatif

| Ã‰lÃ©ment                             | Ã€ faire |
|------------------------------------|--------|
| **Choix mÃ©triques**                | Pertinentes pour la santÃ©, perfs, erreurs |
| **Scrape interval**                | Selon criticitÃ© (15â€“60s) |
| **Choix logs importants**          | Erreurs, warnings, actions critiques |
| **Concepts clÃ©s observabilitÃ©**    | 3 piliers (Metrics, Logs, Traces), alerting fiable, dashboards ciblÃ©s |

************************

## Alertes


### ğŸ§± 1. INFRASTRUCTURE (Serveur / OS / Docker / VM / Kubernetes)

| ğŸ” **MÃ©trique** | ğŸ”” **Pourquoi câ€™est critique** | âœ… **Alerte conseillÃ©e** |
|----------------|-------------------------------|--------------------------|
| `CPU usage` | Surcharge = lenteurs, crashs | CPU > 80% pendant 2-5 min |
| `RAM usage` | Si elle explose â†’ OOM kill | RAM > 80% |
| `Disk usage` | Disque plein = logs perdus, DB en panne | Disque > 90% |
| `Disk IO` | Trop dâ€™Ã©critures = performances dÃ©gradÃ©es | IO latency > 100ms |
| `Load average` | Charge systÃ¨me gÃ©nÃ©rale | Load > nb CPU cores |
| `Network errors` | Paquets perdus = microcoupures | > 1% dâ€™erreurs rÃ©seau |
| `Container restarts` | Crash loop, bugs frÃ©quents | > 3 redÃ©marrages/5 min |
| `File descriptors` | FDs Ã©puisÃ©s = blocages dâ€™apps | > 80% de limite max |

ğŸ“¦ Exporteurs utiles :  
- `node-exporter`, `cadvisor`, `docker-exporter`, `kubelet`, `blackbox-exporter`

---

### ğŸ§  2. APPLICATIONS (API / Frontend / Workers / Services)

| ğŸ” **MÃ©trique** | ğŸ”” **Pourquoi câ€™est critique** | âœ… **Alerte conseillÃ©e** |
|----------------|-------------------------------|--------------------------|
| `HTTP error rate` (4xx, 5xx) | Bugs / crashs utilisateurs | > 5% de 5xx sur 5 min |
| `Request duration` | Lenteurs = mauvaise UX | p95 > 1s |
| `Request rate` | Pics anormaux = attaque ? | Augmentation soudaine |
| `Timeouts` | Comms internes HS (DB, Redisâ€¦) | > X timeouts/min |
| `Queue length` | SystÃ¨me engorgÃ© (jobs async) | File > 100 jobs |
| `DB error rate` | ProblÃ¨mes d'accÃ¨s base | > 1% dâ€™erreurs SQL |
| `Cache hit rate` | Trop de cache misses = lenteur | Hit rate < 80% |

ğŸ“¦ Clients Prometheus :  
- Express / Fastify / NestJS: `prom-client`  
- Django / Flask: `prometheus-client`  
- Spring Boot: `/actuator/prometheus`

---

### ğŸ“œ 3. LOGS STRUCTURÃ‰S (via Loki + Promtail)

| ğŸ” **Type de log** | ğŸ”” **Pourquoi câ€™est critique** | âœ… **LogQL Ã  alerter** |
|-------------------|-------------------------------|------------------------|
| `Error`, `Exception`, `Fatal` | Stack traces = crashs | `count_over_time({job="app"} |= "error" [1m]) > 5` |
| `OutOfMemory`, `OOM` | Processus killÃ© | `{job="app"} |= "OutOfMemory"` |
| `Database error`, `SQL error` | Connexion / requÃªtes invalides | `|= "SQLSTATE"` |
| `Unauthorized`, `403`, `401` | Tentatives d'accÃ¨s interdit | `|= "401"` |
| `Panic`, `Segfault` | Crash niveau Go / C++ | `|= "panic"` |
| `Timeout` | Perte de dÃ©pendances | `|= "timeout"` |
| `Too many requests` | DÃ©bordement, throttling | `|= "429"` |

---

### ğŸ§‘â€ğŸ’¼ 4. BUSINESS / FONCTIONNEL (optionnel mais ğŸ§ )

| ğŸ” **MÃ©trique** | ğŸ”” **Pourquoi câ€™est utile** | âœ… **Alerte** |
|----------------|-----------------------------|--------------|
| Nombre de commandes par min | Chute = bug fonctionnel | < X commandes sur 10 min |
| Paiements Ã©chouÃ©s | ProblÃ¨me Stripe / PayPal | > 5% dâ€™Ã©checs |
| Inscription utilisateurs | Trop peu = bug ou crash | < 1/min |

---

### ğŸ”” Quelles alertes sont **essentielles en prod** ?

| âœ… Alerte | ğŸ’¥ Impact | ğŸ›  Source |
|----------|-----------|----------|
| CPU > 90% | Surcharge | Prometheus |
| RAM > 90% | Crashs / OOM | Prometheus |
| Disk > 90% | DB/logs figÃ©s | Prometheus |
| HTTP 5xx > 5% | Service KO | Prometheus |
| `error` logs x5 en 1min | Crash / bug code | Loki |
| Container restart loop | Crash app | Prometheus |
| DB down / connection refused | Backend KO | Logs + metrics |
| Load average > cores | Saturation | Prometheus |

---

### ğŸ§° Dashboard Grafana Ã  mettre en place

| Dashboard | Contenu |
|----------|---------|
| ğŸ”§ Infra | CPU, RAM, Disk, Restarts par container |
| ğŸŒ API | HTTP rate, error rate, latence |
| ğŸ’¾ DB | Latence requÃªtes, erreurs, connexions |
| ğŸ” SÃ©curitÃ© | Tentatives Ã©chouÃ©es, logs 403/401 |
| ğŸ“œ Logs | Vue filtrÃ©e `error`, `panic`, `fatal` |
| ğŸ“Š Alertes | Tableau de toutes les alertes en cours |


---

## Logging / Monitoring (Production-ready)

### âœ… 1. **Centralisation multi-services / multi-containers**
**Quoi ?**
Assure-toi que **tous tes services**, microservices, conteneurs, et outils loguent vers Loki via Promtail ou un autre agent (ex: Fluent Bit).

**Pourquoi**
> Pour voir toutes les erreurs, performances et Ã©vÃ©nements critiques au mÃªme endroit.

---

### âœ… 2. **Conservation & rotation des logs**
**Quoi ?**
Configurer Loki pour stocker les logs plus longtemps (ex: 7, 15 ou 30 jours) + activer la **rotation automatique** pour Ã©viter que Ã§a explose en taille.

**Pourquoi**
> Pour conserver lâ€™historique utile, respecter les normes RGPD/ISO, et Ã©viter que Loki consomme tout lâ€™espace disque.

---

### âœ… 3. **SÃ©curitÃ© & accÃ¨s**
**Quoi ?**
- Authentification sur Grafana (SSO, OAuth, etc.)
- Lecture seule pour certains utilisateurs
- SÃ©curiser les endpoints Loki, Prometheus, Alertmanager avec du HTTPS et auth (via NGINX ou Traefik par ex.)

**Pourquoi**
> Pour protÃ©ger lâ€™accÃ¨s aux logs sensibles en prod, Ã©viter les fuites ou les erreurs humaines.

---

### âœ… 4. **Dashboards par domaine**
**Quoi ?**
CrÃ©er des **dashboards ciblÃ©s** :
- ğŸ”§ Backend (latence, erreurs, logs `ERROR`)
- ğŸ¨ Frontend (erreurs JS, requÃªtes API)
- ğŸ›¢ï¸ Database (temps de rÃ©ponse, logs de requÃªtes lentes)
- ğŸš¦ Infra (Docker, CPU/RAM, rÃ©seau)

**Pourquoi**
> Pour isoler rapidement les bugs ou les lenteurs, et faire des analyses efficaces.

---

### âœ… 5. **Alertes fines & auto-rÃ©solution**
**Quoi ?**
- CrÃ©er des alertes ciblÃ©es par microservice
- Ajouter des labels (env, service, team)
- Utiliser lâ€™**auto-resolve** pour Ã©viter les spams dâ€™alertes
- DÃ©finir des **schedules** (alerte seulement en heures ouvrÃ©es, etc.)

**Pourquoi**
> Pour ne pas surcharger les Ã©quipes et garder des alertes vraiment utiles.

---

### âœ… 6. **Backups & Haute DisponibilitÃ©**
**Quoi ?**
- Sauvegarder les donnÃ©es Prometheus et Loki
- RÃ©pliquer les composants critiques
- Optionnel : utiliser Loki en mode **distributed** (avec Cortex/Tempo pour les traces)

**Pourquoi**
> Pour ne **pas perdre les donnÃ©es** si crash et garantir un **SLA Ã©levÃ©**.

---

### âœ… 7. **Traces (bonus : observabilitÃ© complÃ¨te ğŸ”)**
**Quoi ?**
Ajouter **Grafana Tempo ou Jaeger** pour suivre une requÃªte de bout en bout.

**Pourquoi**
> Tu vois non seulement quand une erreur sâ€™est produite, mais **exactement oÃ¹ dans le code elle sâ€™est produite.**
